diff -rupN linux-yocto-3.19-patched/arch/x86/syscalls/syscall_32.tbl linux-yocto-3.19-patched/arch/x86/syscalls/syscall_32.tbl
--- linux-yocto-3.19-patched/arch/x86/syscalls/syscall_32.tbl	2017-11-27 09:31:05.519879523 -0800
+++ linux-yocto-3.19-patched/arch/x86/syscalls/syscall_32.tbl	2017-11-29 09:22:55.161399291 -0800
@@ -365,3 +365,5 @@
 356	i386	memfd_create		sys_memfd_create
 357	i386	bpf			sys_bpf
 358	i386	execveat		sys_execveat			stub32_execveat
+359 i386    amt_mem_free       sys_amt_mem_free
+360 i386    amt_mem_claimed    sys_amt_mem_claimed
\ No newline at end of file
diff -rupN linux-yocto-3.19-patched/include/linux/syscalls.h linux-yocto-3.19-patched/include/linux/syscalls.h
--- linux-yocto-3.19-patched/include/linux/syscalls.h	2017-11-27 09:31:10.588968781 -0800
+++ linux-yocto-3.19-patched/include/linux/syscalls.h	2017-11-29 09:23:02.521529585 -0800
@@ -882,4 +882,7 @@ asmlinkage long sys_execveat(int dfd, co
 			const char __user *const __user *argv,
 			const char __user *const __user *envp, int flags);
 
+asmlinkage long sys_amt_mem_claimed(void);
+asmlinkage long sys_amt_mem_free(void);
+
 #endif
diff -rupN linux-yocto-3.19-patched/mm/slob.c linux-yocto-3.19-patched/mm/slob.c
--- linux-yocto-3.19-patched/mm/slob.c	2017-11-27 09:31:11.051976934 -0800
+++ linux-yocto-3.19-patched/mm/slob.c	2017-11-29 09:23:11.408686912 -0800
@@ -68,6 +68,9 @@
 #include <linux/list.h>
 #include <linux/kmemleak.h>
 
+#include <linux/linkage.h>
+#include <linux/syscalls.h>	
+
 #include <trace/events/kmem.h>
 
 #include <linux/atomic.h>
@@ -92,6 +95,10 @@ struct slob_block {
 };
 typedef struct slob_block slob_t;
 
+
+/*Used To Capture The Amount of Caimed Memory*/
+long mem_claimed = 0; 
+
 /*
  * All partially free slob pages go on these lists.
  */
@@ -216,49 +223,133 @@ static void slob_free_pages(void *b, int
  */
 static void *slob_page_alloc(struct page *sp, size_t size, int align)
 {
-	slob_t *prev, *cur, *aligned = NULL;
-	int delta = 0, units = SLOB_UNITS(size);
-
+	slob_t *prev, *cur, *aligned = NULL, *tightest_blk = NULL;
+	slob_t *tightest_prev = NULL, *tightest_aligned = NULL;
+	int delta = 0, units = SLOB_UNITS(size), total_needed;
+	int cur_tightness, tightest_delta = 0, tightest_fit = -1;
+	
 	for (prev = NULL, cur = sp->freelist; ; prev = cur, cur = slob_next(cur)) {
-		slobidx_t avail = slob_units(cur);
+		slobidx_t available = slob_units(cur);
 
 		if (align) {
 			aligned = (slob_t *)ALIGN((unsigned long)cur, align);
 			delta = aligned - cur;
 		}
-		if (avail >= units + delta) { /* room enough? */
-			slob_t *next;
 
-			if (delta) { /* need to fragment head to align? */
-				next = slob_next(cur);
-				set_slob(aligned, avail - delta, next);
-				set_slob(cur, delta, aligned);
-				prev = cur;
-				cur = aligned;
-				avail = slob_units(cur);
+		total_needed = units + delta;
+		
+		/*is there enough room*/
+		if (available >= total_needed) {
+			cur_tightness = available - total_needed;
+			/*if tighter fit, or first iteration*/
+			if (tightest_fit > cur_tightness || tightest_blk == NULL) {
+				tightest_blk = cur;
+				tightest_prev = prev;
+				tightest_aligned = aligned;
+				tightest_delta = delta;
+				tightest_fit = cur_tightness;
+				/*if perfect fit, then break out. No need to compare more*/
+				if (tightest_fit == 0)
+					break;
 			}
+		}
+
+		/*if this is the last block*/
+		if (slob_last(cur)) 
+			break;
+	}
+
+	/*Found best-fitted/tightest block. Now, allocate it*/
+	if (tightest_blk != NULL){
+		slob_t *tightest_next;
+		slobidx_t tightest_avail = slob_units(tightest_blk);
+
+		if (tightest_delta) { /* need to fragment head to align? */
+			tightest_next = slob_next(tightest_blk);
+			set_slob(tightest_aligned, 
+					 (tightest_avail - tightest_delta), 
+					 tightest_next);
+			set_slob(tightest_blk, tightest_delta, tightest_aligned);
+			tightest_prev = tightest_blk;
+			tightest_blk = tightest_aligned;
+			tightest_avail = slob_units(tightest_blk);
+		}
 
-			next = slob_next(cur);
-			if (avail == units) { /* exact fit? unlink. */
-				if (prev)
-					set_slob(prev, slob_units(prev), next);
-				else
-					sp->freelist = next;
-			} else { /* fragment */
-				if (prev)
-					set_slob(prev, slob_units(prev), cur + units);
-				else
-					sp->freelist = cur + units;
-				set_slob(cur + units, avail - units, next);
+		tightest_next = slob_next(tightest_blk);
+
+		/* exact fit? unlink. */
+		if (tightest_avail == units) {
+			if (tightest_prev)
+				set_slob(tightest_prev, slob_units(tightest_prev), tightest_next);
+			else
+				sp->freelist = tightest_next;
+		} 
+		/* fragment */
+		else {
+			if (tightest_prev){
+				set_slob(tightest_prev, slob_units(tightest_prev), tightest_blk + units);
+			}
+			else{
+				sp->freelist = tightest_blk + units;
 			}
+			set_slob(tightest_blk + units, tightest_avail - units, tightest_next);
+		}
 
-			sp->units -= units;
-			if (!sp->units)
-				clear_slob_page_free(sp);
-			return cur;
+		sp->units -= units;
+		if (!sp->units)
+			clear_slob_page_free(sp);
+		return tightest_blk;
+	}
+	/*couldn't fit size in any block*/
+	return NULL;
+}
+
+/*
+*This function makes sure that the page has enough space accounting for alignment and
+	returns the tightness metric
+ Go through all of this page's blocks.
+ 	If the required size fits exactly, then go ahead and return
+ 	Otherwise record the 'tightness' of the current block (i.e. spaces avilable - spaces needed)
+ 	If the tightest_fit > current tightness 
+	   	Make tightest_fit = current available slot
+*/
+static int slob_best_fit_page_check(struct page *sp, size_t size, int align)
+{
+	slob_t *prev, *cur, *aligned = NULL, *tightest_blk = NULL;
+	int delta = 0, units = SLOB_UNITS(size), total_needed;
+	int tightest_fit = -1, cur_tightness = -1;
+
+	
+	for (prev = NULL, cur = sp->freelist; ; prev = cur, cur = slob_next(cur)) {
+		slobidx_t available = slob_units(cur);
+
+		if (align) {
+			aligned = (slob_t *)ALIGN((unsigned long)cur, align);
+			delta = aligned - cur;
+		}
+
+		total_needed = units + delta;
+		
+		/*is there enough room*/
+		if (available >= total_needed) {
+			cur_tightness = available - total_needed;
+			/*if tighter fit, or first possible, switch to using that location*/
+			if (tightest_fit > cur_tightness || tightest_blk == NULL) {
+				tightest_blk = cur;
+				tightest_fit = cur_tightness;
+				/*if perfect fit, then just return. No need to compare more*/
+				if (tightest_fit == 0)
+					return tightest_fit;
+			}
+		}
+
+		/*if this is the last block*/
+		if (slob_last(cur)) {
+			/*if there was a valid fit, return the fit metric*/
+			if (tightest_blk != NULL)
+				return tightest_fit;
+			return -1;			
 		}
-		if (slob_last(cur))
-			return NULL;
 	}
 }
 
@@ -267,11 +358,12 @@ static void *slob_page_alloc(struct page
  */
 static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 {
-	struct page *sp;
-	struct list_head *prev;
-	struct list_head *slob_list;
+	struct page *sp = NULL;
+	struct page *tightest_pg = NULL;
+	struct list_head *slob_list = NULL;
 	slob_t *b = NULL;
 	unsigned long flags;
+	int tightest_fit = -1, cur_tightness = -1;
 
 	if (size < SLOB_BREAK1)
 		slob_list = &free_slob_small;
@@ -281,6 +373,7 @@ static void *slob_alloc(size_t size, gfp
 		slob_list = &free_slob_large;
 
 	spin_lock_irqsave(&slob_lock, flags);
+
 	/* Iterate through each partially free page, try to find room */
 	list_for_each_entry(sp, slob_list, lru) {
 #ifdef CONFIG_NUMA
@@ -295,21 +388,33 @@ static void *slob_alloc(size_t size, gfp
 		if (sp->units < SLOB_UNITS(size))
 			continue;
 
-		/* Attempt to alloc */
-		prev = sp->lru.prev;
-		b = slob_page_alloc(sp, size, align);
-		if (!b)
+		cur_tightness = slob_best_fit_page_check(sp, size, align);
+		/*if it doesn't fit in this page, move to the next page*/
+		if(cur_tightness == -1)
 			continue;
-
-		/* Improve fragment distribution and reduce our average
-		 * search time by starting our next search here. (see
-		 * Knuth vol 1, sec 2.5, pg 449) */
-		if (prev != slob_list->prev &&
-				slob_list->next != prev->next)
-			list_move_tail(slob_list, prev->next);
-		break;
+		/*if perfect fit, then break out. No need to compare more*/
+		else if (cur_tightness == 0){
+			tightest_pg = sp;
+			break;
+		}
+		/*if tighter fit, or first possible page*/
+		else if (tightest_fit > cur_tightness || tightest_pg == NULL) {
+			tightest_pg = sp;
+			tightest_fit = cur_tightness;
+		}
+	}
+		
+	/* Attempt to alloc */
+	if(tightest_pg != NULL){
+		b = slob_page_alloc(tightest_pg, size, align);
 	}
+
 	spin_unlock_irqrestore(&slob_lock, flags);
+	/*
+		* For fragmentation metrics
+		* claimed the size
+	*/
+	mem_claimed += size;
 
 	/* Not enough space: must allocate a new page */
 	if (!b) {
@@ -354,6 +459,13 @@ static void slob_free(void *block, int s
 
 	spin_lock_irqsave(&slob_lock, flags);
 
+	/*
+		* For fragmentation metrics
+		* minus the claimed size from mem_free
+	*/
+	if(mem_claimed - size >= 0)
+		mem_claimed -= size;
+
 	if (sp->units + units == SLOB_UNITS(PAGE_SIZE)) {
 		/* Go directly to page allocator. Do not pass slob allocator */
 		if (slob_page_free(sp))
@@ -640,3 +752,31 @@ void __init kmem_cache_init_late(void)
 {
 	slab_state = FULL;
 }
+
+/*
+	* For fragmentation metrics
+	* return the number of bytes claimed
+*/
+asmlinkage long sys_amt_mem_claimed(void){
+	return mem_claimed;
+}
+/*
+	* For fragmentation metrics
+	* get the number of free spots available in ALL lists
+*/
+asmlinkage long sys_amt_mem_free(void){
+	struct page* sp = NULL;
+	unsigned long mem_free = 0;
+
+	list_for_each_entry(sp, &free_slob_small, lru) {
+		mem_free += sp->units;
+	}
+	list_for_each_entry(sp, &free_slob_medium, lru) {
+		mem_free += sp->units;
+	}
+	list_for_each_entry(sp, &free_slob_large, lru) {
+		mem_free += sp->units;
+	}
+	/*mem_free holds number of slob units. Turn it into bytes*/
+	return (mem_free * SLOB_UNIT) - SLOB_UNIT + 1;
+}
\ No newline at end of file
